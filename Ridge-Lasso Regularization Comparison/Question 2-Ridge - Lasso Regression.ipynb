{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA 2 - CS6510 (CS17MTECH11028)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from scipy.optimize import minimize, check_grad\n",
    "from sklearn.metrics import accuracy_score, auc, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 75)\n",
      "(75,)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "m = 150\n",
    "d = 75\n",
    "X = np.random.rand(m, d)\n",
    "print(X.shape)\n",
    "\n",
    "# Setup theta vector\n",
    "theta = np.zeros(d)\n",
    "t_1 = np.random.randint(1,9)\n",
    "theta[:10] = np.append([10]*t_1, [-10]*(10-t_1))\n",
    "print(theta.shape)\n",
    "\n",
    "# Add random noise\n",
    "noise = 0.3162 * np.random.randn(150) + 0\n",
    "\n",
    "# Generate output column\n",
    "y = np.dot(X, theta) + noise\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training, cross-validation and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:80,:]\n",
    "y_train = y[:80]\n",
    "\n",
    "X_cv = X[80:100, :]\n",
    "y_cv = y[80:100]\n",
    "\n",
    "X_test = X[100:150, :]\n",
    "y_test = y[100:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression Cost Function and Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridgeRegCostAndGradient( theta, X, y, lamb):\n",
    "    # Returns the cost function value and gradient (75x1) for each feature\n",
    "    m = X.shape[0]\n",
    "    cost = np.sum(np.power(y - np.dot(X, theta), 2)/(2*m)) +  np.dot(theta.transpose(), theta)*lamb/(2*m)\n",
    "    grad = np.dot(X.transpose(), np.dot(X, theta)-y)/m + lamb*theta/m\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Ridge Regression minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.50401083e+00   1.01122981e+01   1.03138488e+01   9.58312748e+00\n",
      "   9.52587307e+00   1.06364044e+01  -9.77593638e+00  -9.31610479e+00\n",
      "  -9.57064471e+00  -1.04205445e+01  -3.86018941e-01   1.33687749e-01\n",
      "  -2.56188032e-02  -5.14168295e-02  -5.00341505e-02   6.00489465e-02\n",
      "  -3.71010728e-01  -3.69391542e-01  -9.06328852e-02  -1.91659212e-02\n",
      "   1.57670535e-01   2.66520748e-03   2.85405209e-01  -2.98322382e-01\n",
      "   4.61003760e-01  -3.20887784e-02  -1.15574349e-01  -4.69269281e-02\n",
      "  -1.47277157e-01   3.20423530e-01   1.77566068e-01  -2.88887645e-01\n",
      "   5.31632553e-01   1.97139341e-02   6.62415188e-01  -2.53555592e-01\n",
      "  -7.86195261e-01  -1.17354439e-01   1.07007998e+00  -6.44917437e-01\n",
      "   6.46302358e-01   1.04879542e+00  -4.05690355e-01  -4.75552201e-02\n",
      "   2.50946356e-01   4.49496438e-01   1.66794179e-01  -1.23733605e-01\n",
      "  -6.12613386e-01   3.30118339e-01   2.06934858e-02   4.23519068e-01\n",
      "  -1.31163876e-01   2.01654355e-01   1.89509750e-01   1.58015504e-01\n",
      "  -2.72749832e-01   4.09607382e-01   2.63383587e-01   2.57901553e-01\n",
      "  -4.13428140e-01  -1.01731505e+00  -1.76750266e-01   2.22745968e-01\n",
      "   1.21482198e-02   4.26970812e-01  -2.34871095e-01  -5.45175632e-01\n",
      "  -8.68052908e-02  -3.91133684e-01  -4.33901974e-01  -5.45226365e-01\n",
      "   1.39362751e-01  -8.89906529e-01   2.19731225e-01]\n"
     ]
    }
   ],
   "source": [
    "# Best parameter finalized after using cross-validation for various parameters\n",
    "lamb = 0.001\n",
    "theta_0 = np.zeros(theta.shape)\n",
    "res = minimize(ridgeRegCostAndGradient, theta_0, (X_train, y_train, lamb), jac = True, method = 'CG')\n",
    "#print(res)\n",
    "w = res['x']\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Cross-validation set to find best value of lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation for lambda 1 is 0.8526176973021713\n",
      "Cross-validation for lambda 0.5 is 0.8881558647067153\n",
      "Cross-validation for lambda 0.3 is 0.9075284861206969\n",
      "Cross-validation for lambda 0.2 is 0.9206317347037971\n",
      "Cross-validation for lambda 0.1 is 0.9406551598950957\n",
      "Cross-validation for lambda 0.05 is 0.9575642774209345\n",
      "Cross-validation for lambda 0.01 is 0.9790129272059639\n",
      "Cross-validation for lambda 0.005 is 0.9820790592105224\n",
      "Cross-validation for lambda 0.003 is 0.9828742285288816\n",
      "Cross-validation for lambda 0.001 is 0.9835894798802565\n",
      "\n",
      "Ridge Regression R2 Score on test dataset for lambda 0.001 is 0.9865236037180654\n"
     ]
    }
   ],
   "source": [
    "lamb = [1, 0.5, 0.3,0.2, 0.1, 0.05, 0.01, 0.005, 0.003, 0.001]\n",
    "\n",
    "for l in lamb:\n",
    "    res = minimize(ridgeRegCostAndGradient, theta_0, (X_train, y_train, l), jac = True, method = 'CG')\n",
    "    w   = res['x']\n",
    "    y_pred = np.dot(X_cv, w)\n",
    "    m = X_cv.shape[0]\n",
    "    print('Cross-validation for lambda {0} is {1}'.format(l, r2_score(y_cv, y_pred)))\n",
    "    \n",
    "# Test on test dataset\n",
    "final_lamb = 0.001\n",
    "res = minimize(ridgeRegCostAndGradient, theta_0, (X_train, y_train, final_lamb), jac = True, method = 'CG')\n",
    "w   = res['x']\n",
    "y_pred_test = np.dot(X_test, w)\n",
    "m = X_test.shape[0]\n",
    "print('\\nRidge Regression R2 Score on test dataset for lambda {0} is {1}'.format(final_lamb, r2_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Ridge Regression\n",
    "\n",
    "1. None of the 75 co-efficients are 0. Thus, all the co-efficients with true value have been estimated as non-zero.\n",
    "2. 3-4 co-efficients are in the range of 10^-2.\n",
    "3. This tells that Ridge Regression does not work well when only few of the components matter to the output class.\n",
    "4. Ridge regression does not help in making components to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression Score on test dataset for lambda 0.001 is 0.9972130987562898\n",
      "Co-efficients:\n",
      "[  1.00237394e+01   9.89500371e+00   1.02448083e+01   9.97973912e+00\n",
      "   9.87096506e+00   1.00504247e+01  -9.86407145e+00  -9.81781705e+00\n",
      "  -9.84287834e+00  -9.95446716e+00  -3.19408392e-02  -3.32118830e-02\n",
      "  -1.55706683e-01  -5.78996264e-02   0.00000000e+00  -1.18053347e-01\n",
      "  -2.65476123e-01   0.00000000e+00  -8.29563120e-02  -0.00000000e+00\n",
      "   1.54670464e-01  -5.12065551e-02   1.02923350e-01  -1.32654351e-01\n",
      "   0.00000000e+00  -8.52938432e-03   1.57705359e-02   1.09802406e-01\n",
      "  -5.21613301e-02   2.12651029e-01   1.90416922e-01  -2.92291832e-01\n",
      "   1.62734203e-01  -7.22097015e-02   3.55350521e-02  -0.00000000e+00\n",
      "  -2.55955547e-01  -0.00000000e+00   4.52845342e-01  -3.43396965e-01\n",
      "   1.86687348e-01   1.65651749e-01  -1.44959060e-01  -0.00000000e+00\n",
      "   2.13788060e-01   3.99286675e-02   1.90645143e-01  -2.36321260e-02\n",
      "   0.00000000e+00  -0.00000000e+00   0.00000000e+00   3.73272237e-01\n",
      "  -0.00000000e+00   2.34125777e-01   0.00000000e+00   2.07981404e-01\n",
      "  -5.00808631e-02   1.38682847e-01   0.00000000e+00   2.93803619e-02\n",
      "  -7.58793622e-02  -2.15990573e-01  -5.82651809e-02   0.00000000e+00\n",
      "  -1.86480593e-01   2.29715731e-01  -7.50993036e-02  -1.49652641e-01\n",
      "  -1.37399403e-01  -1.15186418e-01  -3.86133507e-01  -1.28872631e-01\n",
      "   4.07809681e-02  -5.72391914e-01   2.33110591e-01]\n"
     ]
    }
   ],
   "source": [
    "clf = Lasso(alpha=final_lamb)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Lasso Regression Score on test dataset for lambda {0} is {1}'.format(final_lamb, clf.score(X_test, y_test)))\n",
    "print('Co-efficients:')\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Lasso Regression:\n",
    "\n",
    "1. The score Lasso regression with same value of regression parameter (lambda) is higher.\n",
    "2. Around 15 components have zero weight. It is a good improvement over Ridge regression where no component had 0 weight.\n",
    "3. This shows the property of Lasso Regression that it works well when most of the features are not contributing to the output and expect a 0 weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956712746039\n",
      "[  9.29137717  10.05406575   9.6779121    8.66846397  10.34292812\n",
      "  -9.93358548 -10.29422236  -8.87066849 -10.46927695  -9.82405289\n",
      "   0.64245597  -0.397585     0.26363489   0.79079923  -0.09762192\n",
      "  -0.23467183  -0.71437651   0.47149143   0.86826085   0.77079415\n",
      "  -0.25658072   1.18897426  -0.24756031   0.36858452  -0.34360844\n",
      "   0.12992103   0.62685166   0.63231174   0.99884239  -0.08241659\n",
      "  -0.12676361  -0.30477545   0.41390597  -0.41054053   0.71053354\n",
      "   1.17869789  -0.22233989   0.52527025  -0.75955876  -0.298957\n",
      "   0.84622329   0.50225394   0.88888368   0.48619408  -0.54041187\n",
      "   0.01533551  -0.65320636  -0.3711382   -0.31487865   1.2570542\n",
      "   0.86235333   0.54356732  -0.81859463   0.65084397   0.81106435\n",
      "  -0.70356194  -0.87718769  -0.08109026  -0.65897486   0.39333052\n",
      "  -0.0127953   -0.41922138   0.47073213  -0.19574749  -0.86603208\n",
      "   0.32397324  -0.86648646  -0.34351225  -0.83735135   0.62438122\n",
      "  -0.60583903   0.02514896  -0.25727958   0.83006389   0.16244218]\n"
     ]
    }
   ],
   "source": [
    "# Debugging with sklearn ridge regression\n",
    "clf = Ridge(alpha=0.001)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
